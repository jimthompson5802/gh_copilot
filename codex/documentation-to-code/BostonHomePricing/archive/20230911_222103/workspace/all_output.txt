Based on the requirements, we will need the following core classes, functions, and methods:

1. `load_data`: This function will load the data from the CSV file into a pandas DataFrame.
2. `preprocess_data`: This function will handle missing values, outliers, and feature scaling.
3. `evaluate_correlations`: This function will evaluate the correlations between each feature and the target feature, and remove features with low correlation.
4. `split_data`: This function will split the data into a training set and a test set.
5. `train_models`: This function will train multiple regression models on the training data.
6. `evaluate_models`: This function will evaluate the models on the test data and print the loss value.
7. `save_models`: This function will save the trained models to disk.

Now, let's write the code for each of these functions.

main.py
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import pickle
import os
from preprocess import preprocess_data
from correlations import evaluate_correlations

def load_data(filepath):
    # Load data from CSV file into a pandas DataFrame
    df = pd.read_csv(filepath)
    return df

def split_data(df, target, test_size=0.2):
    # Split the data into a training set and a test set
    X = df.drop(target, axis=1)
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    return X_train, X_test, y_train, y_test

def train_models(X_train, y_train):
    # Train multiple regression models
    models = {
        "Linear Regression": LinearRegression(),
        "Lasso": Lasso(),
        "ElasticNet": ElasticNet()
    }
    for name, model in models.items():
        model.fit(X_train, y_train)
    return models

def evaluate_models(models, X_test, y_test):
    # Evaluate the models and print the loss value
    for name, model in models.items():
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        print(f"{name} MSE: {mse}")

def save_models(models, directory="models"):
    # Save the trained models to disk
    if not os.path.exists(directory):
        os.makedirs(directory)
    for name, model in models.items():
        filepath = os.path.join(directory, f"{name.replace(' ', '_').lower()}.pkl")
        with open(filepath, "wb") as f:
            pickle.dump(model, f)

def main():
    # Load data
    df = load_data("../../generate-synthetic-data/workspace/data/housing2.csv")

    # Preprocess data
    df = preprocess_data(df)

    # Evaluate correlations and remove low-correlation features
    df = evaluate_correlations(df, "MEDV")

    # Split data
    X_train, X_test, y_train, y_test = split_data(df, "MEDV")

    # Train models
    models = train_models(X_train, y_train)

    # Evaluate models
    evaluate_models(models, X_test, y_test)

    # Save models
    save_models(models)

if __name__ == "__main__":
    main()
```

preprocess.py
```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

def preprocess_data(df):
    # Fill missing values with the median of the respective column
    df.fillna(df.median(), inplace=True)

    # Handle outliers using IQR method
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

    # Feature Scaling
    scaler = StandardScaler()
    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

    return df
```

correlations.py
```python
def evaluate_correlations(df, target):
    # Evaluate the correlations between each feature and the target feature
    correlations = df.corr()[target]

    # Remove features with low correlation
    low_correlation_features = correlations[correlations.abs() < 0.5].index
    df = df.drop(low_correlation_features, axis=1)

    return df
```

requirements.txt
```
pandas
numpy
scikit-learn
```

This code should be fully functional assuming the correct versions of the required packages are installed. The `requirements.txt` file lists these packages.