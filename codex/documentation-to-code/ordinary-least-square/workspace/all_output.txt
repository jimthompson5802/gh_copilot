Based on the requirements, assumptions, and steps involved in building the OLS regression model, the following core classes, functions, and methods will be necessary:

1. `OLSModel` class:
   - `fit(X, y)`: Method to fit the OLS model to the given dataset.
   - `predict(X)`: Method to predict new values using the trained OLS model.
   - `evaluate(X, y)`: Method to evaluate the model's performance using MSE and R-squared metrics.
   - `visualize(X, y)`: Method to visualize the original data and the best-fitting line of the OLS model.

2. `mean_squared_error` function: Function to calculate the mean squared error between predicted and actual values.
3. `r_squared` function: Function to calculate the R-squared metric for the model's performance.
4. `plot_data` function: Function to plot the original data points and the best-fitting line of the OLS model.

Now, let's proceed with creating the necessary files and implementing the code.

**File 1: ols_model.py**

```python
import numpy as np

class OLSModel:
    def __init__(self):
        self.coefficients = None
    
    def fit(self, X, y):
        # Add a column of ones to X for the intercept term
        X = np.c_[np.ones(X.shape[0]), X]
        
        # Calculate the coefficients using the OLS formula
        self.coefficients = np.linalg.inv(X.T @ X) @ X.T @ y
    
    def predict(self, X):
        # Add a column of ones to X for the intercept term
        X = np.c_[np.ones(X.shape[0]), X]
        
        # Predict the values using the trained model
        return X @ self.coefficients
    
    def evaluate(self, X, y):
        # Predict the values using the trained model
        y_pred = self.predict(X)
        
        # Calculate the mean squared error
        mse = mean_squared_error(y, y_pred)
        
        # Calculate the R-squared metric
        r2 = r_squared(y, y_pred)
        
        return mse, r2
    
    def visualize(self, X, y):
        # Plot the original data points
        plot_data(X, y)
        
        # Add a column of ones to X for the intercept term
        X = np.c_[np.ones(X.shape[0]), X]
        
        # Predict the values using the trained model
        y_pred = X @ self.coefficients
        
        # Plot the best-fitting line of the OLS model
        plt.plot(X[:, 1], y_pred, color='red')
        plt.show()
```

**File 2: metrics.py**

```python
def mean_squared_error(y_true, y_pred):
    # Calculate the mean squared error
    mse = np.mean((y_true - y_pred) ** 2)
    return mse

def r_squared(y_true, y_pred):
    # Calculate the total sum of squares
    tss = np.sum((y_true - np.mean(y_true)) ** 2)
    
    # Calculate the residual sum of squares
    rss = np.sum((y_true - y_pred) ** 2)
    
    # Calculate the R-squared metric
    r2 = 1 - (rss / tss)
    return r2
```

**File 3: visualization.py**

```python
import matplotlib.pyplot as plt

def plot_data(X, y):
    # Plot the original data points
    plt.scatter(X, y)
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Original Data')
    plt.show()
```

**File 4: main.py**

```python
from ols_model import OLSModel
import numpy as np

# Create a sample dataset
X = np.array([[1, 2], [3, 4], [5, 6]])
y = np.array([3, 5, 7])

# Create an instance of the OLSModel class
model = OLSModel()

# Fit the OLS model to the dataset
model.fit(X, y)

# Predict new values using the trained model
X_new = np.array([[7, 8], [9, 10]])
y_pred = model.predict(X_new)
print("Predicted values:", y_pred)

# Evaluate the model's performance
mse, r2 = model.evaluate(X, y)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Visualize the original data and the best-fitting line
model.visualize(X, y)
```

**File 5: requirements.txt**

```
numpy
matplotlib
```

Please note that the code provided is a basic implementation of the OLS regression model and may not handle all possible scenarios or edge cases. It serves as a starting point for further enhancements and customization based on specific requirements.